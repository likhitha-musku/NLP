{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TF-IDF_ContentModeration_ExpertData.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YopqJ_L9vx8F",
        "outputId": "4b67851a-da00-4916-d7d2-8aef5c6a2773"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09QndBvNUZb",
        "outputId": "6bd3e6ee-8890-4bc6-9fe6-022d23ade6c2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg5GWJeBu9EQ"
      },
      "source": [
        "#build a text corpus with all the words present in these tweets. For this, we can try to pick up each tweet in the dataset break down into words and try and append into a single list.\n",
        "\n",
        "def corpus_build(column):\n",
        "    \"\"\"Function to create a corpus list for all the words present in the tweets.Pass in the \n",
        "    dataframe column\"\"\"\n",
        "    text_corpus = []\n",
        "    for i in column.str.split():\n",
        "        for word in i:\n",
        "            text_corpus.append(word)\n",
        "    return text_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzF4DWRSOQJ7"
      },
      "source": [
        "import re\n",
        "\n",
        "#Function to remove the links in the text\n",
        "def remove_url(input):\n",
        "    \"\"\"Function to remove the URLs present in the text. Feed in the text data as input to function\"\"\"\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQeM1i6-O_Hp"
      },
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(input1):\n",
        "    \"\"\"To remove all the punctuations present in the text. Input the text to the function\"\"\"\n",
        "    table = str.maketrans('','',string.punctuation)\n",
        "    return input1.translate(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMTXJKENPFie"
      },
      "source": [
        "def remove_b(text):\n",
        "    text  = text[1:]\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAbcP5zlPcZz"
      },
      "source": [
        "def remove_linebreaks(input1):\n",
        "    \"\"\"Function to remove the line breaks  present in the text. Feed in the text data as input to function\"\"\"\n",
        "    text = re.compile(r'\\n')\n",
        "    return text.sub(r' ',input1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZlIG6S6SySc"
      },
      "source": [
        "def remove_stopwords(input1):\n",
        "    \"\"\"Function to remove the stopwords present in the text. Feed in the text data as input to function\"\"\"\n",
        "    words = []\n",
        "    for word in input1:\n",
        "        if word not in stopwords.words('english'):\n",
        "            words.append(word)\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VoGAGKRS9g9"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def lemma_wordnet(input1):\n",
        "    \"\"\"Lemmatization function\"\"\"\n",
        "    return [lem.lemmatize(w) for w in input1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJIGBuUXTJ38"
      },
      "source": [
        "def combine_text(input1):\n",
        "    \"\"\"Function to combine the list words\"\"\"\n",
        "    combined = ' '.join(input1)\n",
        "    return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF7rmiHnsZKk"
      },
      "source": [
        "def runAnalysis(df):\n",
        "  corpus_build(df['Text'])\n",
        "  print('step2')\n",
        "  df['Text'] = df['Text'].apply(lambda x: remove_url(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: remove_punctuation(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: remove_b(x))\n",
        "\n",
        "  df['Text'] = df['Text'].str.lower()\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: remove_linebreaks(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: lemma_wordnet(x))\n",
        "\n",
        "  df['Text'] = df['Text'].apply(lambda x: combine_text(x))\n",
        "\n",
        "  print(df['Text'])\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  response = vectorizer.fit_transform(df['Text'])\n",
        "\n",
        "  terms = vectorizer.get_feature_names()\n",
        "\n",
        "  # sum tfidf frequency of each term through documents\n",
        "  sums = response.sum(axis=0)\n",
        "\n",
        "  # connecting term to its sums frequency\n",
        "  data = []\n",
        "  for col, term in enumerate(terms):\n",
        "      data.append( (term, sums[0,col] ))\n",
        "\n",
        "  ranking = pd.DataFrame(data, columns=['term','rank'])\n",
        "\n",
        "  ranking_sort = ranking.sort_values('rank', ascending=False)\n",
        "\n",
        "  ranking_top20 = ranking_sort[:20]\n",
        "  print(ranking_top20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgAQI8UNv5nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b91992c-5c0d-4389-8a97-bcca2f438872"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "rankingtop20 = []\n",
        "directory = \"/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth\"\n",
        "for file in os.listdir(directory):\n",
        "  print(os.path.join(directory, file))\n",
        "  df = pd.read_csv(os.path.join(directory, file))\n",
        "  tmpRanking = runAnalysis(df)\n",
        "  rankingtop20.append(tmpRanking)\n",
        "\n",
        "print(rankingtop20)\n",
        "\n",
        "#for i in range (0,1):\n",
        "#  df=pd.read_csv(f'/content/gdrive/Shareddrives/Content Moderation/data/New Non Experts Dataset/MainNonExpertFiles/nonExpertTweetTexts_{i}.csv',error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertApril.csv\n",
            "step2\n",
            "0        wexe2x80x99re going need ten thousand contact ...\n",
            "1        must see help others see face life behind numb...\n",
            "2        vaccine save million life every year immunizin...\n",
            "3        rapid spread covid19 show happen dont protecti...\n",
            "4        chinese student model creative colorful way re...\n",
            "                               ...                        \n",
            "12681       total test covid19 per 1000 people apr 15 2020\n",
            "12682                                  lgprime wearing one\n",
            "12683                      jandrews01 thank continues goal\n",
            "12684    friend real life asked needed vitamin delivere...\n",
            "12685    kerrship fatimaamanat danielstadlb shirinstroh...\n",
            "Name: Text, Length: 12686, dtype: object\n",
            "              term        rank\n",
            "19886           rt  330.550628\n",
            "5425       covid19  210.375200\n",
            "1681           amp  137.371507\n",
            "22683        thank  123.100086\n",
            "10338       health  114.147779\n",
            "15702          new  101.956730\n",
            "15598         need   93.586971\n",
            "16721          one   87.825063\n",
            "17445       people   85.512936\n",
            "3950          case   78.591145\n",
            "22617      testing   73.171396\n",
            "5283   coronavirus   70.465065\n",
            "22688       thanks   69.078118\n",
            "23035         time   68.248438\n",
            "25146         work   65.853400\n",
            "12960         know   64.974093\n",
            "13497         like   64.137577\n",
            "23098        today   61.292266\n",
            "5972           day   60.125197\n",
            "14112         many   59.241156\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertMay.csv\n",
            "step2\n",
            "0        rt mikebloomberg contact tracing find virus li...\n",
            "1        rt foreignaffairs impatience reinforced miscon...\n",
            "2        rt drkellyhenning pleasure speaking wcexaminer...\n",
            "3        first time month past 3 day deathsday nyc covi...\n",
            "4        saving life instrumental restoring economic ac...\n",
            "                               ...                        \n",
            "12282    susannalharris stevengoldleaf doctorate since ...\n",
            "12283    angierasmussen miller even claim talk alien in...\n",
            "12284    maustermuhle tell scientist say thank maybe on...\n",
            "12285                               favorite town michigan\n",
            "12286    think going confuse amp anger people ohio cont...\n",
            "Name: Text, Length: 12287, dtype: object\n",
            "           term        rank\n",
            "19863        rt  338.167451\n",
            "5480    covid19  194.346392\n",
            "10372    health  121.695598\n",
            "1778        amp  118.516875\n",
            "17320    people   99.149610\n",
            "15593       new   97.888310\n",
            "16623       one   88.857679\n",
            "22617     thank   84.001341\n",
            "15494      need   80.625140\n",
            "13981      many   75.487159\n",
            "4064       case   71.945549\n",
            "13386      like   68.409789\n",
            "5479      covid   67.339855\n",
            "18462    public   65.964469\n",
            "17021  pandemic   62.610278\n",
            "25017      work   60.614798\n",
            "12867      know   60.256567\n",
            "22558   testing   60.222868\n",
            "24077   vaccine   60.092551\n",
            "6036        day   60.025528\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertJune.csv\n",
            "step2\n",
            "0        rt dronibee learned aapolicyforums black light...\n",
            "1        rt lipiroy stevekornacki testing situation imp...\n",
            "2        rt gregggonsalves first newhaven protest yeste...\n",
            "3        rt mclemoremr xe2x80x9calmost people color for...\n",
            "4        rt nnkinsi jasminesolola beautifully wrote ear...\n",
            "                               ...                        \n",
            "18038    rt jasonjwilde nicely done whitehouse niagaraf...\n",
            "18039    fantastic nsv conference siena kudos organizer...\n",
            "18040                                        whoisbentelab\n",
            "18041    2nd generation affinofiles last great high thr...\n",
            "18042    xd1x81xd0xbaxd0xb0xd1x87xd0xb0xd1x82xd1x8c sky...\n",
            "Name: Text, Length: 18043, dtype: object\n",
            "           term        rank\n",
            "24636        rt  485.372502\n",
            "6847    covid19  264.607747\n",
            "5034       case  162.197707\n",
            "21406    people  156.769060\n",
            "2189        amp  152.457180\n",
            "19130       new  142.893071\n",
            "12672    health  142.339137\n",
            "27990     thank  121.658772\n",
            "17086      many  115.746070\n",
            "20531       one  115.708916\n",
            "27995    thanks  110.585528\n",
            "6846      covid  103.460473\n",
            "18994      need  101.799128\n",
            "16384      like  100.921802\n",
            "17260      mask   98.577061\n",
            "26739     state   98.227626\n",
            "22875    public   96.280338\n",
            "28546     today   94.530482\n",
            "28452      time   91.553685\n",
            "21031  pandemic   89.180316\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertJuly.csv\n",
            "step2\n",
            "0              new ebola specie reported first time decade\n",
            "1        thoughtprovoking presentation public health cr...\n",
            "2        rt phegov aspr roll new incident response fram...\n",
            "3        ebola outbreak drc end call international effo...\n",
            "4        thank taraniasim providing behindthestage assi...\n",
            "                               ...                        \n",
            "29692    dwuhlfelderlaw parent choice theyre told go ba...\n",
            "29693    xf0x9fx94xa5xf0x9fx94xa5xf0x9fx94xa5xf0x9fx94x...\n",
            "29694    14 3540 everpopular masked hamster made anothe...\n",
            "29695    7 reviewed evidence mask mucked messaging feba...\n",
            "29696    1010 vaccine xe2x80xa6 encouraging data prelim...\n",
            "Name: Text, Length: 29697, dtype: object\n",
            "          term        rank\n",
            "33856       rt  768.069845\n",
            "9189   covid19  359.555799\n",
            "3037       amp  238.611939\n",
            "29672   people  220.372466\n",
            "6834      case  214.396043\n",
            "28461      one  209.645534\n",
            "38428   thanks  207.748934\n",
            "23552     many  201.485894\n",
            "26436      new  196.598614\n",
            "26258     need  195.503180\n",
            "17394   health  190.326177\n",
            "22558     like  182.281614\n",
            "38423    thank  179.962232\n",
            "16049      get  178.358007\n",
            "9188     covid  174.124342\n",
            "39055     time  172.966271\n",
            "23791     mask  164.866779\n",
            "34505   school  161.687502\n",
            "21709     know  153.775350\n",
            "10103      day  150.129503\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertAugust.csv\n",
            "step2\n",
            "0        doubt big tobaccoxe2x80x99s commitment keeping...\n",
            "1        wonderful article life alice hamilton pioneer ...\n",
            "2        interesting study thelancet week one pill lead...\n",
            "3        globalhealthbmj achieving universalhealthcare ...\n",
            "4        statin indicated people high risk stroke heart...\n",
            "                               ...                        \n",
            "31731    fantastic project proposed f1000 postpublicati...\n",
            "31732    msl nthat nerve wrecking watching congrats ima...\n",
            "31733    msl neven top notch nasa scientist dont mess l...\n",
            "31734    curiosity nquote day jpl medium relation journ...\n",
            "31735    curiosity nat caltech watching curiosity land ...\n",
            "Name: Text, Length: 31736, dtype: object\n",
            "          term        rank\n",
            "36241       rt  820.505597\n",
            "9960   covid19  331.264123\n",
            "31805   people  266.485665\n",
            "3144       amp  236.814103\n",
            "30456      one  208.670620\n",
            "41235    thank  193.985831\n",
            "28109     need  193.183333\n",
            "28294      new  192.739273\n",
            "9958     covid  190.074509\n",
            "18555   health  178.563175\n",
            "41938     time  172.557073\n",
            "17214      get  172.457806\n",
            "24183     like  171.865182\n",
            "7358      case  169.574236\n",
            "23278     know  168.636222\n",
            "25220     many  165.190121\n",
            "36981   school  163.732125\n",
            "41119  testing  158.002832\n",
            "41107     test  156.910735\n",
            "41645    think  151.593058\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertSeptember.csv\n",
            "step2\n",
            "0        meantime reduced depending data nytimes covid1...\n",
            "1        mask definitely mandated herenpossible sign li...\n",
            "2        buckle fellow scientist lot scicomm work isnt ...\n",
            "3                                   look fact wherever may\n",
            "4                                            sadly america\n",
            "                               ...                        \n",
            "27253    reached ccpsfl covid site reopeningschools doe...\n",
            "27254                                       many face palm\n",
            "27255    realdonaldtrump proud boy bull rush blm take b...\n",
            "27256    petrovadmitri fitness instantiated property in...\n",
            "27257    day 2 brownsph question pvd folksnnwhere go be...\n",
            "Name: Text, Length: 27258, dtype: object\n",
            "                  term        rank\n",
            "32754               rt  713.397891\n",
            "8935           covid19  275.526362\n",
            "37212            thank  245.723951\n",
            "2785               amp  210.783967\n",
            "28727           people  206.806771\n",
            "27565              one  181.025664\n",
            "39715          vaccine  180.160559\n",
            "8933             covid  170.400441\n",
            "38526            trump  160.091331\n",
            "20970             know  149.899760\n",
            "21838             like  148.416616\n",
            "25452             need  145.269867\n",
            "25622              new  144.710787\n",
            "16766           health  143.884443\n",
            "22785             many  141.397576\n",
            "31188  realdonaldtrump  139.808134\n",
            "37773             time  137.652199\n",
            "28216         pandemic  128.807593\n",
            "37218           thanks  124.326820\n",
            "15540              get  122.823384\n",
            "/content/gdrive/Shareddrives/Content Moderation/data/LinkAnalysisandWordCount/expertByMonth/expertMarch.csv\n",
            "step2\n",
            "0        burundian pal colleague amazing job presenting...\n",
            "1        thrilled brilliant emily gurley sharing class ...\n",
            "2        story nightmarish similarly aged son got foreh...\n",
            "3        cnbc television interview amp doctoryasmin dis...\n",
            "4        could incubation period might shorter case cou...\n",
            "                               ...                        \n",
            "11207    25 result indicate efficient transmission sars...\n",
            "11208    federal govt nearly enough testing sarscov2 cd...\n",
            "11209    sudden global disease outbreak isnxe2x80x99t k...\n",
            "11210    weekxe2x80x99s niecetoobomb news family viral ...\n",
            "11211    sardisgazette christinepolon1 dont know wrote ...\n",
            "Name: Text, Length: 11212, dtype: object\n",
            "              term        rank\n",
            "17944           rt  342.879044\n",
            "5019       covid19  231.336607\n",
            "4870   coronavirus  135.055537\n",
            "1732           amp  125.182954\n",
            "9485        health  109.015679\n",
            "3686          case  105.473445\n",
            "14142          new  102.303596\n",
            "20420        thank   96.173398\n",
            "14058         need   90.685784\n",
            "15755       people   87.696044\n",
            "20384      testing   71.841825\n",
            "20703         time   68.409691\n",
            "15113          one   66.662386\n",
            "20779        today   62.162992\n",
            "10316    important   57.404601\n",
            "5522           day   57.383830\n",
            "11775         know   56.904424\n",
            "20428       thanks   56.719421\n",
            "12230         like   56.229436\n",
            "18189     sarscov2   56.211111\n",
            "[None, None, None, None, None, None, None]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}